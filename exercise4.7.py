# -*- coding: utf-8 -*-
"""understanding-creating-word-embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/programminghistorian/jekyll/blob/gh-pages/assets/understanding-creating-word-embeddings/understanding-creating-word-embeddings.ipynb

## Preparing Your Corpus
"""

import re                                   # for regular expressions
import os                                   # to look up operating system-based info
import string                               # to do fancy things with strings
import glob                                 # to locate a specific file type
from pathlib import Path                    # to access files in other directories
import gensim                               # to access Word2Vec
# to access Gensim's flavor of Word2Vec
from gensim.models import Word2Vec
import pandas as pd                         # to sort and organize data
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# ======================
# TEXT PREPARATION
# ======================

# Set file paths
novel_path = "Barnaby-Rudge.txt"  # Ensure this file exists in your directory

# Load and preprocess text


def load_and_clean_text(file_path):
    """Load and clean text from a file"""
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # Basic cleaning
    text = text.replace('\n', ' ').replace('\r', ' ')  # Remove line breaks

    return text

# Text cleaning function


def clean_text(text):
    """Tokenize and clean text"""
    # Lowercase
    tokens = text.split()
    tokens = [t.lower() for t in tokens]

    # Remove punctuation
    re_punc = re.compile('[%s]' % re.escape(string.punctuation))
    tokens = [re_punc.sub('', token) for token in tokens]

    # Remove non-alphabetic tokens
    tokens = [token for token in tokens if token.isalpha()]

    return tokens


# Load and process the novel
print("Loading and processing text...")
raw_text = load_and_clean_text(novel_path)
cleaned_tokens = clean_text(raw_text)

# Split into sentences (crude method)
sentences = []
current_sentence = []
for token in cleaned_tokens:
    current_sentence.append(token)
    if token in ['mr', 'mrs', 'said', 'chapter'] or len(current_sentence) > 15:
        sentences.append(current_sentence)
        current_sentence = []
if current_sentence:
    sentences.append(current_sentence)

print(
    f"Processed {len(sentences)} sentences with {len(cleaned_tokens)} total tokens")

# ======================
# MODEL TRAINING
# ======================

# Train the model
print("\nTraining Word2Vec model...")
model = Word2Vec(
    sentences=sentences,
    vector_size=150,    # Dimensionality of word vectors
    window=8,           # Context window size
    min_count=5,        # Ignore words with <5 occurrences
    workers=4,          # Parallel processing
    epochs=15,          # Training iterations
    sg=1                # Skip-gram (1) or CBOW (0)
)

# Save the model
model.save("barnaby_rudge_word2vec.model")
print("Model saved as 'barnaby_rudge_word2vec.model'")

# ======================
# EXPLORATORY ANALYSIS
# ======================

# Vocabulary check
novel_words = ["barnaby", "grip", "gordon", "haredale",
               "riots", "london", "chester", "simon", "dolly"]
print("\nVocabulary check:")
for word in novel_words:
    if word in model.wv.key_to_index:
        print(
            f"  '{word}' in vocabulary (index: {model.wv.key_to_index[word]})")
    else:
        print(f"  '{word}' not in vocabulary")

# Similarity analysis


def show_similar_words(model, word, topn=10):
    """Display similar words with similarity scores"""
    if word in model.wv:
        print(f"\nWords similar to '{word}':")
        similar = model.wv.most_similar(word, topn=topn)
        for word, score in similar:
            print(f"{word}: {score:.4f}")
    else:
        print(f"'{word}' not in vocabulary")


# Key characters and themes
show_similar_words(model, "barnaby")
show_similar_words(model, "grip")
show_similar_words(model, "gordon")
show_similar_words(model, "riots")

# Relationships
print("\nRelationship tests:")
print("Barnaby and Grip similarity:", model.wv.similarity("barnaby", "grip"))
print("Gordon and riots similarity:", model.wv.similarity("gordon", "riots"))
print("Haredale and Chester similarity:",
      model.wv.similarity("haredale", "chester"))

# Analogies
print("\nAnalogies:")
# Barnaby is to Grip as Gordon is to ?
print("Barnaby:Grip :: Gordon:?", model.wv.most_similar(
    positive=["gordon", "grip"],
    negative=["barnaby"],
    topn=3
))

# ======================
# VALIDATION & EVALUATION
# ======================

# Test word pairs specific to Barnaby Rudge
test_pairs = [
    ("barnaby", "innocent"),
    ("gordon", "protestant"),
    ("riots", "violence"),
    ("haredale", "revenge"),
    ("london", "city"),
    ("crowd", "mob"),
    ("grip", "raven"),
    ("chester", "deceit")
]

results = []
for word1, word2 in test_pairs:
    if word1 in model.wv and word2 in model.wv:
        similarity = model.wv.similarity(word1, word2)
        results.append({
            "Word Pair": f"{word1}-{word2}",
            "Similarity": similarity
        })
    else:
        missing = word1 if word1 not in model.wv else word2
        print(f"Pair '{word1}-{word2}' skipped: '{missing}' not in vocabulary")

# Create and display results dataframe
results_df = pd.DataFrame(results)
print("\nSimilarity Test Results:")
print(results_df.sort_values("Similarity", ascending=False))

# Save results
results_df.to_csv("barnaby_similarity_results.csv", index=False)
print("Results saved to 'barnaby_similarity_results.csv'")

# ======================
# VISUALIZATION
# ======================

# Visualize word embeddings


def plot_embeddings(model, words):
    """Plot word vectors in 2D space"""
    # Get vectors
    vectors = [model.wv[word] for word in words]

    # Reduce dimensionality
    pca = PCA(n_components=2)
    points = pca.fit_transform(vectors)

    # Plot
    plt.figure(figsize=(12, 8))
    plt.scatter(points[:, 0], points[:, 1], color='steelblue', alpha=0.7)

    # Add labels
    for i, word in enumerate(words):
        plt.annotate(word,
                     xy=(points[i, 0], points[i, 1]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     fontsize=12)

    plt.title("Barnaby Rudge Word Embeddings", fontsize=16)
    plt.xlabel("PCA Dimension 1", fontsize=14)
    plt.ylabel("PCA Dimension 2", fontsize=14)
    plt.grid(alpha=0.2)
    plt.tight_layout()
    plt.savefig("barnaby_embeddings.png", dpi=300)
    plt.show()


# Plot key words
plot_words = ["barnaby", "grip", "gordon", "riots", "haredale", "chester",
              "dolly", "simon", "london", "mob", "violence", "protestant",
              "innocent", "revenge", "deceit"]
available_words = [word for word in plot_words if word in model.wv]

print("\nPlotting word embeddings...")
plot_embeddings(model, available_words)

print("\nAnalysis complete!")

"""## Next Steps

Here are some resources if you would like to learn more about word vectors:

- The Women [Writers Vector Toolkit](https://wwp.northeastern.edu/lab/wwvt/index.html) is a web interface for exploring word vectors, accompanied by glossaries, sources, case studies, and sample assignments. This toolkit includes links to a [GitHub repository with RMD walkthroughs](https://github.com/NEU-DSG/wwp-public-code-share/tree/main/WordVectors) with code for training word2vec models in R, as well as [download and resources on preparing text corpora](https://wwp.northeastern.edu/lab/wwvt/resources/downloads/index.html).

- The [Women Writers Project Resources](https://wwp.northeastern.edu/outreach/resources/index.html) page has guides on: searching your corpus, corpus analysis and preparation, model validation and assessment, and other materials for working with word vectors.

- [Link to other PH tutorial in draft]

_This walkthrough was written on November 16, 2022 using Python 3.8.3 and Gensim 4.2.0_
"""
